#!/usr/bin/env python3
"""
Script simple para probar que todo funciona
"""

def test_imports():
    """Probar que todas las importaciones funcionan"""
    print("üß™ Probando importaciones...")
    
    try:
        import torch
        print("‚úÖ PyTorch")
        
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        print("‚úÖ Transformers")
        
        from datasets import Dataset
        print("‚úÖ Datasets")
        
        import yaml
        print("‚úÖ PyYAML")
        
        import os
        import sys
        sys.path.append('.')
        
        from utils.data_utils import get_label_names
        print("‚úÖ Utils")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en importaciones: {e}")
        return False

def test_model_loading():
    """Probar carga de modelo"""
    print("\nü§ñ Probando carga de modelo...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        
        model_name = "bert-base-multilingual-cased"
        print(f"Cargando: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("‚úÖ Tokenizer cargado")
        
        model = AutoModelForTokenClassification.from_pretrained(
            model_name, 
            num_labels=9
        )
        print("‚úÖ Modelo cargado")
        
        return True, tokenizer, model
        
    except Exception as e:
        print(f"‚ùå Error cargando modelo: {e}")
        return False, None, None

def test_dataset_creation():
    """Probar creaci√≥n de dataset simple"""
    print("\nüìä Probando creaci√≥n de dataset...")
    
    try:
        from datasets import Dataset, DatasetDict
        
        # Datos de ejemplo
        sample_data = {
            'tokens': [
                ['Juan', 'vive', 'en', 'Madrid'],
                ['Apple', 'es', 'una', 'empresa'],
                ['Mar√≠a', 'trabaja', 'en', 'Google']
            ],
            'ner_tags': [
                [1, 0, 0, 5],  # B-PER, O, O, B-LOC
                [3, 0, 0, 0],  # B-ORG, O, O, O
                [1, 0, 0, 3]   # B-PER, O, O, B-ORG
            ]
        }
        
        dataset = Dataset.from_dict(sample_data)
        print(f"‚úÖ Dataset creado con {len(dataset)} ejemplos")
        
        return True, dataset
        
    except Exception as e:
        print(f"‚ùå Error creando dataset: {e}")
        return False, None

def test_tokenization():
    """Probar tokenizaci√≥n"""
    print("\nüî§ Probando tokenizaci√≥n...")
    
    try:
        success, tokenizer, model = test_model_loading()
        if not success:
            return False
        
        success, dataset = test_dataset_creation()
        if not success:
            return False
        
        # Probar tokenizaci√≥n simple
        sample_tokens = ['Juan', 'vive', 'en', 'Madrid']
        
        tokenized = tokenizer(
            sample_tokens,
            is_split_into_words=True,
            truncation=True,
            padding=True,
            max_length=128
        )
        
        print("‚úÖ Tokenizaci√≥n exitosa")
        print(f"   Input tokens: {len(sample_tokens)}")
        print(f"   Tokenized: {len(tokenized['input_ids'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en tokenizaci√≥n: {e}")
        return False

def test_simple_training():
    """Probar un mini-entrenamiento"""
    print("\nüèãÔ∏è Probando mini-entrenamiento...")
    
    try:
        from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification
        import sys
        sys.path.append('.')
        from utils.data_utils import tokenize_and_align_labels
        
        success, tokenizer, model = test_model_loading()
        if not success:
            return False
            
        success, dataset = test_dataset_creation() 
        if not success:
            return False
        
        # Configuraci√≥n simple para tokenizaci√≥n
        config = {
            'dataset': {
                'text_column': 'tokens',
                'label_column': 'ner_tags'
            },
            'model': {
                'max_length': 128
            }
        }
        
        # Tokenizar dataset
        def tokenize_function(examples):
            return tokenize_and_align_labels(examples, tokenizer, config)
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        print("‚úÖ Dataset tokenizado")
        
        # Configuraci√≥n de entrenamiento m√≠nima
        training_args = TrainingArguments(
            output_dir="./test_output",
            num_train_epochs=1,
            per_device_train_batch_size=1,
            save_steps=1000,
            logging_steps=100,
            remove_unused_columns=False
        )
        
        data_collator = DataCollatorForTokenClassification(
            tokenizer=tokenizer,
            padding=True
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
        )
        
        print("‚úÖ Trainer configurado")
        print("‚ö†Ô∏è  Entrenamiento real omitido en prueba")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en configuraci√≥n de entrenamiento: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Funci√≥n principal"""
    print("üöÄ PRUEBAS SIMPLES DEL SISTEMA")
    print("=" * 50)
    
    tests_passed = 0
    total_tests = 4
    
    if test_imports():
        tests_passed += 1
    
    if test_model_loading()[0]:
        tests_passed += 1
    
    if test_dataset_creation()[0]:
        tests_passed += 1
    
    if test_simple_training():
        tests_passed += 1
    
    print("\n" + "=" * 50)
    print(f"üìä RESULTADOS: {tests_passed}/{total_tests} pruebas pasaron")
    
    if tests_passed == total_tests:
        print("üéâ ¬°Todas las pruebas pasaron!")
        print("El sistema est√° listo para entrenar modelos NER")
    else:
        print("‚ö†Ô∏è  Algunas pruebas fallaron")
        print("Revisa los errores arriba")
    
    return tests_passed == total_tests

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)